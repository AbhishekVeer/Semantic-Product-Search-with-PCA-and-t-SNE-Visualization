# Semantic-Product-Search-with-PCA-and-t-SNE-Visualization
Build an AI-powered product search engine using Transformer embeddings, ChromaDB, and visualization with PCA &amp; t-SNE.

# ğŸ›ï¸ Smart Product Search: How It Works  

Imagine you walk into a store and say to the shopkeeper:  
ğŸ‘‰ *â€œI need a camera for vlogging.â€*  

Now, how does our system act like that smart shopkeeper?  

---

## 1ï¸âƒ£ Transformers: Understanding Context  
We use **Transformer models** (like SentenceTransformer) to process text.  
Unlike older models that looked at words one by one, transformers understand **context**.  

Example:  
- *â€œAppleâ€ (fruit)* ğŸ â‰  *â€œAppleâ€ (company)* ğŸ’»  
Transformers know the difference because they look at **surrounding words**.  

---

## 2ï¸âƒ£ Embeddings: Turning Text into Numbers  
Computers canâ€™t understand words directly.  
So, each sentence or product description is converted into a **vector** (a list of numbers) called an **embedding**.  

ğŸ‘‰ Think of embeddings as **coordinates on a map**:  
- â€œRunning sneakersâ€ and â€œJogging shoesâ€ end up close together.  
- â€œHeadphonesâ€ and â€œLaptopâ€ are far apart.  

This is how the computer â€œunderstandsâ€ meaning.  

---

## 3ï¸âƒ£ Vector Database (ChromaDB)  
Once all products are converted into embeddings, we store them in **ChromaDB**.  

ğŸ’¡ Why not a normal database?  
- Normal DBs search by exact **keywords**.  
- ChromaDB searches by **meaning**.  

So even if the query is *â€œmarathon sneakersâ€*, ChromaDB can match with **Nike Air Zoom** or **Adidas Ultraboost**.  

---


## What Algo runs behind Vetor Database (there are many distance search algorithms - Cosine Similarity is just one of them)

### Cosine Similarity: The Matchmaker 

Now, how does ChromaDB actually decide **which product is closest in meaning**?  

It uses **Cosine Similarity Formula:** 

$$
\text{similarity}(A, B) = \frac{A \cdot B}{||A|| \times ||B||}
$$

Where:  
- **A** = the query embedding (your search as numbers)  
- **B** = a stored product embedding  
- **A Â· B** = dot product (overlap) between A and B  
- **||A||, ||B||** = lengths of the vectors  

ğŸ‘‰ In simple words:  
- If two arrows point in the **same direction**, they mean the same thing.  
- If they point in different directions, they are unrelated.  

**Example:**  

Query = *â€œrunning shoesâ€*  
- Product A: *â€œmarathon sneakersâ€* â†’ similarity = **0.95** âœ…  
- Product B: *â€œwireless headphonesâ€* â†’ similarity = **0.12** âŒ  

So Product A will be ranked higher.  

---

## 4ï¸âƒ£ PCA vs t-SNE: Visualizing Embeddings  

Embeddings live in **hundreds of dimensions** (far more than humans can imagine).  
To make sense of them, we reduce them into 2D or 3D for visualization.  
Thatâ€™s where **PCA** and **t-SNE** come in.  

---

### ğŸ”· PCA (Principal Component Analysis)  
- **What it does**: Finds the main â€œdirectionsâ€ of variation in the data.  
- This is a technique to reduce the size of embeddings (usually hundreds of dimensions) into just 2 or 3 dimensions so humans can visualize them. PCA keeps the â€œmain patternsâ€ in the data while dropping small details. 
- Itâ€™s like compressing a big photo but still being able to see the main picture. 
- Strengths:  
  - Keeps the **global structure** of the data.  
  - Fast and simple.  
- Limitations:  
  - Sometimes spreads apart points that are actually very similar in meaning.  

ğŸ‘‰ Example: With PCA, all types of shoes may appear somewhat close,  
but running shoes vs. casual sneakers may not cluster tightly.  

---

### ğŸ”¶ t-SNE (t-distributed Stochastic Neighbor Embedding)  
- **What it does**: Focuses on preserving **local neighborhoods**.  
- It ensures that points that were close in high dimensions stay close in 2D.  
- Strengths:  
  - Great at showing **clusters of similar items**.  
  - More human-intuitive: â€œthings that belong together, appear together.â€  
- Limitations:  
  - Slower and more computationally heavy.  
  - Sometimes distorts global distances.  

ğŸ‘‰ Example: With t-SNE, â€œrunning shoesâ€ and â€œmarathon sneakersâ€ will appear very close,  
even if the bigger picture of *all footwear* gets a bit distorted.  

---

### ğŸ§© How They Work Together  
- **PCA** gives the **big map** â€” showing how all products are spread out overall.  
- **t-SNE** zooms into **clusters** â€” showing small groups of very similar items.  

Thatâ€™s why we often look at **both plots**:  
- PCA â†’ â€œWhere am I in the whole city?â€ ğŸ—ºï¸  
- t-SNE â†’ â€œWho are my closest neighbors?â€ ğŸ˜ï¸  

---

## 5ï¸âƒ£ The Search Process  

1. User types a query (e.g., *â€œBest laptop for travelâ€*).  
2. Transformer converts it into an **embedding**.  
3. ChromaDB compares it with stored product embeddings.  
4. Finds the **closest match**.  
   âœ… *Dell XPS 13 â†’ Lightweight laptop with long battery life.*  

---

## 6ï¸âƒ£ Why Return Metadata + Description?  
- **Metadata** â†’ Product name (*Dell XPS 13*).  
- **Document** â†’ Product description (*Lightweight laptop with strong performanceâ€¦*).  

Together, they form a complete human-readable answer.  

---

## 7ï¸âƒ£ End-to-End Flow  

**User Query â†’ Embedding â†’ ChromaDB â†’ Best Match â†’ Return Metadata + Description**  

---

### ğŸ”‘ Summary  
We transformed **text â†’ embeddings â†’ stored in ChromaDB â†’ searched by meaning â†’ returned the best product**.  

And we can even **visualize this â€œsemantic mapâ€** using PCA or t-SNE  
to see how products cluster by meaning.  


Text â†’ Embeddings â†’ Stored in ChromaDB â†’ Searched by Meaning â†’ Best Product Returned âœ…
